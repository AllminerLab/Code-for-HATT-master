{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee63cb-f370-4bda-b060-c05117969dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, scipy.sparse as sp\n",
    "import torch.optim as optim, torch.nn.functional as F\n",
    "import torch_sparse\n",
    "import pickle\n",
    "import config\n",
    "import random\n",
    "from torch_scatter import scatter\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import torch.nn as nn, torch.nn.functional as F\n",
    "import math\n",
    "from torch_geometric.utils import softmax\n",
    "import sklearn.metrics\n",
    "import os\n",
    "from torchinfo import summary\n",
    "args = config2.parse()\n",
    "#device = torch.device('cuda:'+args.cuda if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0acd6-4be8-42b8-8e42-92ab3e4d2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc5e148-76bc-4c94-a58d-d5f0383dfe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_dict_train = defaultdict(set) \n",
    "with open('1101_1108/c_u_train.txt','r') as file:\n",
    "    length = 0\n",
    "    while 1:\n",
    "        line = file.readline()\n",
    "        if(line==''):\n",
    "            break\n",
    "        n = len(line)\n",
    "        s = line[0:n-1].split(',')\n",
    "        #print(s)\n",
    "        length += (len(s)-1)\n",
    "        for j in range(len(s)-1):\n",
    "            #print(s[0],s[j+1])\n",
    "            com_dict_train[s[0]].add(int(s[j+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9678b0-68e6-4f2c-8c75-ad526b3b5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_com_dict = {}\n",
    "with open('1101_1108/c_u_train.txt','r') as file:\n",
    "    while 1:\n",
    "        line = file.readline()\n",
    "        if(line==''):\n",
    "            break\n",
    "        n = len(line)\n",
    "        s = line[0:n-1].split(',')\n",
    "        for j in range(len(s)-1):\n",
    "            user_com_dict[int(s[j+1])] = int(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3497e2-206c-4ac3-a4ab-16d60986652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"1101_1108/com_user_train.pkl\", \"wb\")\n",
    "pickle.dump(com_dict_train, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34df9220-1542-4187-87f2-382123cefce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_dict_test = defaultdict(set) \n",
    "with open('1101_1108/c_u_test.txt','r') as file:\n",
    "    length = 0\n",
    "    while 1:\n",
    "        line = file.readline()\n",
    "        if(line==''):\n",
    "            break\n",
    "        n = len(line)\n",
    "        s = line[0:n-1].split(',')\n",
    "        length+=(len(s)-1)\n",
    "        for j in range(len(s)-1):\n",
    "            #print(s[0],s[j+1])\n",
    "            com_dict_test[s[0]].add(int(s[j+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1220fe48-4479-4e08-b31b-a0b555267d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"1101_1108/com_user_test.pkl\", \"wb\")\n",
    "pickle.dump(com_dict_test, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa86b335-7cf0-4b24-90dd-e10fa5643f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_1={}\n",
    "i=0\n",
    "with open('1101_1108/c_u_train.txt','r') as file:\n",
    "    while 1:\n",
    "        line = file.readline()\n",
    "        if(line==''):\n",
    "            break\n",
    "        n = len(line)\n",
    "        s = line[:n-1].split(',')\n",
    "        list_1 = []\n",
    "        list_1.append(int(s[0])+args.user_number)\n",
    "        for j in range(len(s)-1):\n",
    "            list_1.append(int(s[j+1]))\n",
    "        dict_1[i]=set(list_1)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb5d08c-7bc3-4dcb-bfd4-f682fb0d8469",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"1101_1108/com_user_train_edge.pkl\", \"wb\")\n",
    "pickle.dump(dict_1, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50c15f-54f1-4def-8e80-1ba7bf0d1812",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_2={}\n",
    "i=0\n",
    "with open('1101_1108/u_a.txt','r') as file:\n",
    "    while 1:\n",
    "        line = file.readline()\n",
    "        if(line==''):\n",
    "            break\n",
    "        n = len(line)\n",
    "        s = line[:n-1].split(',')\n",
    "        list_1 = []\n",
    "        list_1.append(int(s[0]))\n",
    "        if(user_com_dict.__contains__(int(s[0]))):\n",
    "            #print(s[0])\n",
    "            list_1.append(int(user_com_dict[int(s[0])])+args.user_number)\n",
    "        for j in range(len(s)-1):\n",
    "            list_1.append(int(s[j+1])+args.user_number+args.community_number)\n",
    "        dict_2[i]=set(list_1)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf0413a-f41b-49c2-9ef9-af2df7db4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"1101_1108/user_attr_edge.pkl\", \"wb\")\n",
    "pickle.dump(dict_2, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca47f4-f81d-4d37-bb60-e538b027194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_cu = []\n",
    "i=0\n",
    "with open('1101_1108/c_u_train.txt','r') as file:\n",
    "    while 1:\n",
    "        line = file.readline()\n",
    "        if(line==''):\n",
    "            break\n",
    "        n = len(line)\n",
    "        s = line[:n-1].split(',')\n",
    "        list_1 = [0]*args.user_number\n",
    "        for j in range(1,len(s)-1):\n",
    "            list_1[int(s[j])]=1\n",
    "        mat_cu.append(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d8c12-4005-4548-b03f-75fb5fe61439",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_ua = []\n",
    "with open('1101_1108/u_a.txt','r') as file:\n",
    "    while 1:\n",
    "        line = file.readline()\n",
    "        if(line==''):\n",
    "            break\n",
    "        n = len(line)\n",
    "        s = line[:n-1].split(',')\n",
    "        list_1 = [0]*args.attribute_number\n",
    "        for j in range(1,len(s)-1):\n",
    "            list_1[int(s[j])]=1\n",
    "        mat_ua.append(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2cfaf6-32bd-49b9-a6cc-da1c6c8d7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_ca = []\n",
    "with open('1101_1108/c_a.txt','r') as file:\n",
    "    while 1:\n",
    "        line = file.readline()\n",
    "        if(line==''):\n",
    "            break\n",
    "        n = len(line)\n",
    "        s = line[:n-1].split(',')\n",
    "        list_1 = [0]*19\n",
    "        for j in range(1,len(s)-1):\n",
    "            list_1[int(s[j])]=1\n",
    "        mat_ca.append(list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f145fb42-a5a5-4f8f-8220-f48c125b494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_cu = torch.tensor(mat_cu,  dtype=torch.float)\n",
    "mat_ua = torch.tensor(mat_ua,  dtype=torch.float)\n",
    "mat_ca = torch.tensor(mat_ca,  dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a536f74f-1a63-41cd-b99d-186ed23316a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu, seed\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "def get_data(args):\n",
    "    dataset = args.dataset\n",
    "    print('dataset name: ', dataset)\n",
    "    \n",
    "    #node numbers\n",
    "    user_number = args.user_number\n",
    "    train_user_number = args.train_user_number\n",
    "    test_user_number = args.test_user_number\n",
    "    community_number=args.community_number\n",
    "    attribute_number=args.attribute_number\n",
    "    day_point_number=args.day_point_number\n",
    "    node_number = community_number+attribute_number+train_user_number+test_user_number\n",
    "\n",
    "    #edge and edge numbers\n",
    "    com_user = open(dataset + \"/com_user_train_edge.pkl\",'rb+')  ## community1: user1, user2, user3,...\n",
    "    com_user_edge = pickle.load(com_user)\n",
    "    com_user_edge_num=len(com_user_edge)\n",
    "    args.com_user_edge_num=com_user_edge_num\n",
    "    print(\"community hyperedge number is:\", com_user_edge_num)\n",
    "\n",
    "    user_attr = open(dataset+\"/user_attr_edge.pkl\",'rb+')  ## user1: community, attr1, attr2, attr3,...\n",
    "    user_attr_edge=pickle.load(user_attr)\n",
    "    user_attr_edge_num=len(user_attr_edge)\n",
    "    args.user_attr_edge_num=user_attr_edge_num\n",
    "    print(\"user hyperedge is:\", user_attr_edge_num)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    all_hyper_edge = []\n",
    "\n",
    "\n",
    "    #train data and test data\n",
    "    com_user_train = open(dataset + \"/com_user_train.pkl\",'rb+')\n",
    "    com_user_train = pickle.load(com_user_train)\n",
    "    com_user_test = open(dataset + \"/com_user_test.pkl\",'rb+')\n",
    "    com_user_test = pickle.load(com_user_test)\n",
    "    \n",
    "    \n",
    "    #train and test data\n",
    "    K=args.negative_K\n",
    "    com_user_list_train = []\n",
    "    for i in com_user_train.keys():\n",
    "        for j in com_user_train[i]:\n",
    "            com_user_list_train.append([j, int(i)+args.user_number])\n",
    "    com_user_list_train_true = com_user_list_train.copy()\n",
    "\n",
    "    com_user_list_test = []\n",
    "    for i in com_user_test.keys():\n",
    "        for j in com_user_test[i]:\n",
    "            com_user_list_test.append([j, int(i)+args.user_number])\n",
    "    com_user_list_test_true = com_user_list_test.copy()\n",
    "\n",
    "    for i in range(len(com_user_list_test)):\n",
    "        neg = random.randint(args.user_number, args.user_number+args.community_number-1)\n",
    "        while 1:\n",
    "            if (neg == com_user_list_test[i][1]):\n",
    "                neg = random.randint(args.user_number, args.user_number+args.community_number-1)\n",
    "            else:\n",
    "                break\n",
    "        com_user_list_test.append([com_user_list_test[i][0], neg])\n",
    "    \n",
    "\n",
    "    for i in range(len(com_user_list_train)):\n",
    "        for j in range(K):\n",
    "            neg = random.randint(args.user_number, args.user_number+args.community_number-1)\n",
    "            while 1:\n",
    "                if (neg == com_user_list_train[i][1]):\n",
    "                    neg = random.randint(args.user_number, args.user_number+args.community_number-1)\n",
    "                else:\n",
    "                    break\n",
    "            com_user_list_train.append([com_user_list_train[i][0], neg])\n",
    "\n",
    "\n",
    "    \n",
    "    test_label=[]\n",
    "    for i in range(2* len(com_user_list_test_true)):\n",
    "        if i <len(com_user_list_test_true):\n",
    "            test_label.append(1.0)\n",
    "        else:\n",
    "            test_label.append(0.0)\n",
    "    test_label=np.array(test_label)\n",
    "            \n",
    "    train_label=[]\n",
    "    for i in range((1+K)* len(com_user_list_train_true)):\n",
    "        if i <len(com_user_list_train_true):\n",
    "            train_label.append(1.0)\n",
    "        else:\n",
    "            train_label.append(0.0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "    G={}\n",
    "    G['community_user'] = com_user_edge\n",
    "    G['user_attribute'] = user_attr_edge\n",
    "\n",
    "    return   G, com_user_list_test, com_user_list_train, test_label, train_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4511797-bb6a-4ce6-b4a5-7b15c68f96a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name:  1101_1108\n",
      "community hyperedge number is: 453\n",
      "user hyperedge is: 8442\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "model_name = args.model_name\n",
    "\n",
    "node_type=['user','community','attribute']\n",
    "edge_type=['community_user','user_attribute']\n",
    "\n",
    "G, com_user_list_test, com_user_list_train, test_label, train_label = get_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75782d4b-f6ac-4147-9fa3-191a115c0084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff22e4-3cd2-4116-ac17-13e00377ded3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nf = open(\"1101_1108/com_user_list_test.pkl\", \"wb\")\\npickle.dump(com_user_list_test, f)\\nf.close()\\n\\nf = open(\"1101_1108/com_user_list_train.pkl\", \"wb\")\\npickle.dump(com_user_list_train, f)\\nf.close()\\n\\nf = open(\"1101_1108/test_label.pkl\", \"wb\")\\npickle.dump(test_label, f)\\nf.close()\\n\\nf = open(\"1101_1108/train_label.pkl\", \"wb\")\\npickle.dump(train_label, f)\\nf.close()\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "f = open(\"1101_1108/com_user_list_test.pkl\", \"wb\")\n",
    "pickle.dump(com_user_list_test, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"1101_1108/com_user_list_train.pkl\", \"wb\")\n",
    "pickle.dump(com_user_list_train, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"1101_1108/test_label.pkl\", \"wb\")\n",
    "pickle.dump(test_label, f)\n",
    "f.close()\n",
    "\n",
    "f = open(\"1101_1108/train_label.pkl\", \"wb\")\n",
    "pickle.dump(train_label, f)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e14dfc-66c4-4d3a-97ca-d2fd193867a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fp = open(\"1101_1108/com_user_list_test.pkl\", \"rb+\")\n",
    "com_user_list_test = pickle.load(fp)\n",
    "fp.close()\n",
    "fp = open(\"1101_1108/com_user_list_train.pkl\", \"rb+\")\n",
    "com_user_list_train = pickle.load(fp)\n",
    "fp.close()\n",
    "fp = open(\"1101_1108/test_label.pkl\", \"rb+\")\n",
    "test_label = pickle.load(fp)\n",
    "fp.close()\n",
    "fp = open(\"1101_1108/train_label.pkl\", \"rb+\")\n",
    "train_label = pickle.load(fp)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9827325-c682-489c-b42a-01d1d789dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_user_tensor_test = (torch.tensor(np.array(com_user_list_test),dtype=torch.long)).t().contiguous()\n",
    "com_user_tensor_train = (torch.tensor(np.array(com_user_list_train),dtype=torch.long)).t().contiguous()\n",
    "train_label=torch.tensor(np.array(train_label),dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f9898-22bd-476d-a533-d4e3d07caf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial(G, args, node_type, edge_type, mat_cu, mat_ua, mat_ca, unseen=None):\n",
    "\n",
    "    G_all={}\n",
    "    z=0\n",
    "    for i in edge_type:\n",
    "        for j in range(len(G[i])):\n",
    "            G_all[z] = G[i][j]\n",
    "            z+=1\n",
    "\n",
    "    G = G_all.copy()\n",
    "    if unseen is not None:\n",
    "        unseen = set(unseen)\n",
    "        # remove unseen nodes\n",
    "        for e, vs in G.items():\n",
    "            G[e] =  list(set(vs) - unseen)\n",
    "    node_number= args.user_number+ args.community_number + args.attribute_number\n",
    "    args.node_num = node_number\n",
    "    N, M = node_number, len(G)\n",
    "\n",
    "    indptr, indices, data = [0], [], []\n",
    "    for e, vs in G.items():\n",
    "        indices += vs\n",
    "        data += [1] * len(vs)\n",
    "        indptr.append(len(indices))\n",
    "    H = sp.csc_matrix((data, indices, indptr), shape=(N, M), dtype=int).tocsr()\n",
    "\n",
    "    (row, col), value = torch_sparse.from_scipy(H)\n",
    "    V, E = row, col\n",
    "    args.edge_num=max(E)+1\n",
    "\n",
    "    edge_input_length= [args.user_attr_edge_num, args.com_user_edge_num]\n",
    "    node_input_length = [args.user_number, args.community_number, args.attribute_number]\n",
    "\n",
    "    V_raw_index_type=[0 for i in range(args.user_number)]+ [1 for i in range(args.community_number)]+[2 for i in range(args.attribute_number)]\n",
    "    args.V_raw_index_type=torch.tensor(V_raw_index_type, dtype=torch.long)\n",
    "    E_raw_index_type=[0 for i in range(args.user_attr_edge_num)]+ [1 for i in range(args.com_user_edge_num)]\n",
    "    args.E_raw_index_type=torch.tensor(E_raw_index_type, dtype=torch.long)\n",
    "\n",
    "    args.edge_type=edge_type\n",
    "    args.node_type=node_type\n",
    "\n",
    "    a=0\n",
    "    edge_input_length_raw = []\n",
    "    for i in range(len(edge_input_length)):\n",
    "        edge_input_length_raw.append(a+edge_input_length[i])\n",
    "        a=a + edge_input_length[i]\n",
    "    args.edge_input_length_raw = edge_input_length_raw\n",
    "\n",
    "    b=0\n",
    "    node_input_length_raw=[]\n",
    "    for i in range(len(node_input_length)):\n",
    "        node_input_length_raw.append(b+node_input_length[i])\n",
    "        b= b+ node_input_length[i]\n",
    "    args.node_input_length_raw = node_input_length_raw\n",
    "\n",
    "    V_class=[]\n",
    "    V_class_index_0, V_class_index_1, V_class_index_2=[],[],[]\n",
    "    for i in range(V.shape[0]):\n",
    "        if V[i] <node_input_length_raw[0]:\n",
    "            V_class.append(0) #user\n",
    "            V_class_index_0.append(i)\n",
    "        elif node_input_length_raw[0]<= V[i] <node_input_length_raw[1]:\n",
    "            V_class.append(1)#community\n",
    "            V_class_index_1.append(i)\n",
    "        elif node_input_length_raw[1]<= V[i] <node_input_length_raw[2]:\n",
    "            V_class.append(2)#attribute\n",
    "            V_class_index_2.append(i)\n",
    "\n",
    "    E_class=[]\n",
    "    E_class_index_0, E_class_index_1=[],[]\n",
    "    for i in range(E.shape[0]):\n",
    "        if E[i]<edge_input_length_raw[0]:\n",
    "            E_class.append(0) #user-attr\n",
    "            E_class_index_0.append(i)\n",
    "        elif edge_input_length_raw[0]<=E[i]<edge_input_length_raw[1]:\n",
    "            E_class.append(1) #com-user\n",
    "            E_class_index_1.append(i)\n",
    "\n",
    "\n",
    "    args.V_class=torch.tensor(V_class,dtype=torch.long)\n",
    "    args.E_class=torch.tensor(E_class,dtype=torch.long)\n",
    "\n",
    "    args.V_class_index_0=torch.tensor(V_class_index_0,dtype=torch.long)\n",
    "    args.V_class_index_1=torch.tensor(V_class_index_1,dtype=torch.long)\n",
    "    args.V_class_index_2=torch.tensor(V_class_index_2,dtype=torch.long)\n",
    "\n",
    "    args.E_class_index_0=torch.tensor(E_class_index_0,dtype=torch.long)\n",
    "    args.E_class_index_1=torch.tensor(E_class_index_1,dtype=torch.long)\n",
    "\n",
    "    args.V=V\n",
    "    args.E=E\n",
    "    V, E = V.to(device), E.to(device)\n",
    "    mat_ca = mat_ca.to(device)\n",
    "    print(device)\n",
    "    print(mat_ca.device)\n",
    "    args.edge_input_length=edge_input_length_raw\n",
    "    args.node_input_length=node_input_length_raw\n",
    "\n",
    "    args.dataset_dict={'hypergraph':G,'n':N,'features':torch.randn(N,args.input_dim)}\n",
    "\n",
    "    nhid = args.nhid\n",
    "    nhead = args.nhead\n",
    "    dropout_ratio = args.dropout\n",
    "    E_class_index=torch.unsqueeze(torch.cat((args.E_class_index_0,args.E_class_index_1) ,0),1)\n",
    "    args.E_class_index  =E_class_index.repeat(1,nhead)\n",
    "    V_class_index=torch.unsqueeze(torch.cat((args.V_class_index_0,args.V_class_index_1,args.V_class_index_2) ,0) ,1)\n",
    "    args.V_class_index= V_class_index.repeat(1,nhead)\n",
    "\n",
    "\n",
    "    model = HATT(args, args.input_dim, nhid, args.out_dim, nhead, dropout_ratio, V, E,  edge_type, node_type, mat_cu, mat_ua, mat_ca)\n",
    "    optimiser = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    model.to(device)\n",
    "    return model, optimiser, G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bbe55-18e1-458b-a6a2-30253322d30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def glorot(tensor):\n",
    "    if tensor is not None:\n",
    "        stdv = math.sqrt(6.0 / (tensor.size(-2) + tensor.size(-1)))\n",
    "        tensor.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "        \n",
    "def normalize_l2(X):\n",
    "    \"\"\"Row-normalize  matrix\"\"\"\n",
    "    rownorm = X.detach().norm(dim=1, keepdim=True)\n",
    "    scale = rownorm.pow(-1)\n",
    "    scale[torch.isinf(scale)] = 0.\n",
    "    X = X * scale\n",
    "    return X\n",
    "\n",
    "class hattConv(nn.Module):\n",
    "    def __init__(self, args, in_channels, out_channels, heads, negative_slope=0.2, device=device, dropout = 0):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_channels, heads * out_channels))\n",
    "        self.bias = nn.Parameter(torch.Tensor(heads * out_channels))\n",
    "        #self.W = nn.Linear(in_channels, heads * out_channels, bias=True)\n",
    "        #stdv = math.sqrt(6.0/(self.in_channels+self.heads * self.out_channels))\n",
    "        #nn.init.uniform_(self.W.weight.data, -stdv, stdv)\n",
    "        #nn.init.uniform_(self.W.bias.data, -stdv, stdv)\n",
    "        \n",
    "        self.att_v_user=nn.Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_v_community = nn.Parameter(torch.Tensor(1, heads, out_channels))\n",
    "        self.att_v_attribute = nn.Parameter(torch.Tensor(1, heads, out_channels))\n",
    "\n",
    "        self.att_e_com = nn.Parameter(torch.Tensor(1, heads, out_channels))    #每个社团的注意力机制\n",
    "        self.att_e_user = nn.Parameter(torch.Tensor(1, heads, out_channels))   #每个user的注意力机制\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        self.args = args\n",
    "        self.edge_num = args.edge_num\n",
    "        self.node_num = args.node_num\n",
    "        self.edge_type = args.edge_type\n",
    "        self.node_type = args.node_type\n",
    "        self.edge_input_length = args.edge_input_length\n",
    "        self.node_input_length = args.node_input_length\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.V_raw_index_type= (args.V_raw_index_type).to(device)\n",
    "        self.V_class=(args.V_class).to(device)\n",
    "        self.E_class=(args.E_class).to(device)\n",
    "        self.V_class_index=(args.V_class_index).to(device)\n",
    "        self.E_class_index=(args.E_class_index).to(device)\n",
    "        self.V_class_index_0 =(args.V_class_index_0).to(device)\n",
    "        self.V_class_index_1 =(args.V_class_index_1).to(device)\n",
    "        self.V_class_index_2 =(args.V_class_index_2).to(device)\n",
    "        self.E_class_index_0 =(args.E_class_index_0).to(device)\n",
    "        self.E_class_index_1 =(args.E_class_index_1).to(device)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels, self.out_channels, self.heads)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        stdv =  math.sqrt(6./(self.in_channels+self.heads * self.out_channels))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        self.bias.data.uniform_(-stdv, stdv)\n",
    "        glorot(self.att_v_user)\n",
    "        glorot(self.att_v_community)\n",
    "        glorot(self.att_v_attribute)\n",
    "        glorot(self.att_e_com)\n",
    "        glorot(self.att_e_user)\n",
    "\n",
    "        \n",
    "    def forward(self, X, vertex, edges):\n",
    "        H, C = self.heads, self.out_channels\n",
    "        X0 = X.matmul(self.weight) + self.bias\n",
    "        X = X0.view(self.node_num, H, C)\n",
    "        Xve = X[vertex]\n",
    "        X = Xve\n",
    "        Y_e_0 = (torch.index_select(X, 0, self.E_class_index_0) * self.att_e_com).sum(-1)\n",
    "        Y_e_1 = (torch.index_select(X, 0, self.E_class_index_1) * self.att_e_user).sum(-1)\n",
    "        Y_e = torch.cat((Y_e_0, Y_e_1), 0)\n",
    "        #print('1',Y_e.shape)\n",
    "        beta_v = torch.gather(Y_e, 0, self.E_class_index)\n",
    "        beta = self.leaky_relu(beta_v)\n",
    "        beta = softmax(beta, edges, num_nodes=self.edge_num)\n",
    "        beta = beta.unsqueeze(-1)\n",
    "        Ye = Xve * beta\n",
    "        #print('2',Ye.shape)\n",
    "        Ye = (scatter(Ye, edges, dim=0, reduce='sum', dim_size=self.edge_num))\n",
    "        #print('3',Ye.shape)\n",
    "        Ye = F.dropout(Ye, self.dropout, training=self.training)\n",
    "        Y_out = Ye.view(self.edge_num, H * C)\n",
    "        Y_out = self.relu(Y_out)\n",
    "        Ye = Ye[edges]\n",
    "        Y = Ye\n",
    "        X_ve_0 = (torch.index_select(Y, 0, self.V_class_index_0) * self.att_v_user).sum(-1)\n",
    "        X_ve_1 = (torch.index_select(Y, 0, self.V_class_index_1) * self.att_v_attribute).sum(-1)\n",
    "        X_ve_2 = (torch.index_select(Y, 0, self.V_class_index_2) * self.att_v_community).sum(-1)\n",
    "        X_ve = torch.cat((X_ve_0, X_ve_1, X_ve_2), 0)\n",
    "        alpha_e = torch.gather(X_ve, 0, self.V_class_index)\n",
    "        alpha = self.leaky_relu(alpha_e)\n",
    "        alpha = softmax(alpha, vertex, num_nodes=self.node_num)\n",
    "        alpha = alpha.unsqueeze(-1)\n",
    "        Xve = Ye * alpha\n",
    "        Xve = scatter(Xve, vertex, dim=0, reduce='sum', dim_size=self.node_num)  # [N, H, C]\n",
    "        Xve = F.dropout(Xve, self.dropout, training=self.training)\n",
    "        X_out = Xve.view(self.node_num, H * C)\n",
    "        X_out = self.relu(X_out)\n",
    "        return X_out, Y_out\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class HATT(nn.Module):\n",
    "    def __init__(self, args, nfeat, nhid, out_dim, nhead, drop_rate, V, E, edge_type, node_type, mat_cu, mat_ua, mat_ca):\n",
    "\n",
    "        super().__init__()\n",
    "        self.conv_in = hattConv(args, nfeat, nhid, heads=nhead, device=device, dropout = drop_rate)\n",
    "        self.conv_out = hattConv(args, nhid*nhead, nhid, heads=args.out_nhead, device=device, dropout = 0)\n",
    "        \n",
    "        attr_feature = torch.FloatTensor(args.attribute_number, nfeat)\n",
    "        nn.init.xavier_uniform_(attr_feature)\n",
    "        user_feature = torch.matmul(mat_ua, attr_feature)\n",
    "        com_feature = torch.matmul(mat_cu, user_feature)\n",
    "        X = [user_feature, com_feature, attr_feature]\n",
    "        self.X = nn.Parameter(torch.cat((X), 0))\n",
    "        self.V = V\n",
    "        self.E = E\n",
    "        self.edge_type = edge_type\n",
    "        self.node_type = node_type\n",
    "        self.mat_ca = mat_ca\n",
    "        self.lin_out = nn.Linear(nhid *args.out_nhead, out_dim, bias=True)\n",
    "        self.com_embed = nn.Linear(mat_ca.shape[1], out_dim, bias=True)\n",
    "\n",
    "    \n",
    "    def forward(self):\n",
    "\n",
    "        V, E = (self.V) , (self.E)\n",
    "        E_com = self.com_embed(self.mat_ca)\n",
    "        X_1,Y_1 = self.conv_in(self.X, V, E)\n",
    "        X_1,Y_1 = self.conv_out(X_1, V, E)\n",
    "        \n",
    "        X_out = self.lin_out(X_1)\n",
    "        Y_out = self.lin_out(Y_1)\n",
    "\n",
    "        return   X_out, self.X, E_com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c42b2-51e8-46e1-9178-e1947ec0d33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, G = initial(G, args, node_type, edge_type, mat_cu, mat_ua, mat_ca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa3ee3-8fd8-4df4-96d2-c633f366d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#个体对比学习\n",
    "def node_ssl_loss(args, node_embedding, edge_embedding):\n",
    "    # pos and neg for the first graph\n",
    "    user_node_embeddings = node_embedding[:args.user_number]\n",
    "    com_node_embeddings = node_embedding[args.user_number:args.user_number+args.community_number]\n",
    "    user_edge_embeddings = edge_embedding[:args.user_number]\n",
    "    com_edge_embeddings = edge_embedding[args.user_number:args.user_number+args.community_number]\n",
    "    user_node_embeddings_all = node_embedding[:args.user_number]\n",
    "    com_node_embeddings_all = node_embedding[args.user_number:args.user_number+args.community_number]\n",
    "    \n",
    "    norm_user_emb1 = F.normalize(user_edge_embeddings)\n",
    "    norm_user_emb2 = F.normalize(user_node_embeddings)\n",
    "    norm_all_user_emb = F.normalize(user_node_embeddings_all)\n",
    "    pos_score_user = torch.mul(norm_user_emb1, norm_user_emb2).sum(dim=1)\n",
    "    ttl_score_user = torch.matmul(norm_user_emb1, norm_all_user_emb.transpose(0, 1))\n",
    "    pos_score_user = torch.exp(pos_score_user / args.ssl_temp)\n",
    "    ttl_score_user = torch.exp(ttl_score_user / args.ssl_temp).sum(axis=1)\n",
    "    ssl_loss_user = -torch.log(pos_score_user / ttl_score_user).sum()\n",
    "\n",
    "    norm_com_emb1 = F.normalize(com_edge_embeddings)\n",
    "    norm_com_emb2 = F.normalize(com_node_embeddings)\n",
    "    norm_all_com_emb = F.normalize(com_node_embeddings_all)\n",
    "    pos_score_com = torch.mul(norm_com_emb1, norm_com_emb2).sum(dim=1)\n",
    "    ttl_score_com = torch.matmul(norm_com_emb1, norm_all_com_emb.transpose(0, 1))\n",
    "    pos_score_com = torch.exp(pos_score_com / args.ssl_temp)\n",
    "    ttl_score_com = torch.exp(ttl_score_com/ args.ssl_temp).sum(axis=1)\n",
    "    \n",
    "    ssl_loss_com = -torch.log(pos_score_com / ttl_score_com).sum()\n",
    "    ssl_loss = args.ssl_reg * (ssl_loss_user + ssl_loss_com)\n",
    "    \n",
    "\n",
    "    return ssl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba94b0c-331c-4b55-be61-adc093e42e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def com_ssl_loss(args, com_embedding, hatt_embedding):\n",
    "\n",
    "    com_embedding = com_embedding[args.user_number:args.user_number+args.community_number]\n",
    "    norm_com_emb1 = F.normalize(hatt_embedding)\n",
    "    norm_com_emb2 = F.normalize(com_embedding)\n",
    "    norm_all_com_emb = F.normalize(hatt_embedding)\n",
    "    pos_score_com = torch.mul(norm_com_emb1, norm_com_emb2).sum(dim=1)\n",
    "    ttl_score_com = torch.matmul(norm_com_emb1, norm_all_com_emb.transpose(0, 1))\n",
    "    pos_score_com = torch.exp(pos_score_com / args.ssl2_temp)\n",
    "    ttl_score_com = torch.exp(ttl_score_com/ args.ssl2_temp).sum(axis=1)\n",
    "    \n",
    "    ssl_loss_com = -torch.log(pos_score_com / ttl_score_com).sum()\n",
    "    ssl_loss = args.ssl2_reg*ssl_loss_com\n",
    "    \n",
    "\n",
    "    return ssl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66656c68-3aaf-48b8-8597-5a5773c9bdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(Z_1,test_label, user_com_edge_test):\n",
    "    N_tr = args.train_user_number\n",
    "    N_c = args.community_number\n",
    "    N_te = args.test_user_number\n",
    "    N_u = N_tr + N_te\n",
    "    emb = Z_1[:N_u + N_c]\n",
    "    test_predict_label=F.cosine_similarity(emb[user_com_edge_test[0]], emb[user_com_edge_test[1]])\n",
    "    #test_predict_label = F.sigmoid(test_predict_label)\n",
    "    test_predict_label=test_predict_label.detach().numpy()\n",
    "    emb_norm = torch.norm(emb, dim=-1, keepdim=True)\n",
    "    dot = torch.mm(emb, emb.t())\n",
    "    dot_norm = torch.mm(emb_norm, emb_norm.t())\n",
    "    sim = (dot / dot_norm)\n",
    "    \n",
    "    print('test_predict_label',test_predict_label)\n",
    "    print('test_label',test_label)\n",
    "    \n",
    "            \n",
    "    auc=sklearn.metrics.roc_auc_score(test_label,test_predict_label)\n",
    "    ap = sklearn.metrics.average_precision_score(test_label, test_predict_label)\n",
    "    print(\"ap:\",ap)\n",
    "    mea= np.mean(test_predict_label)\n",
    "    acc=sklearn.metrics.accuracy_score(test_label,test_predict_label>mea)\n",
    "    print('acc_test:',acc)\n",
    "    return  auc, test_predict_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18ddbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_auc, test_auc = 0, 0\n",
    "k = int(com_user_tensor_train.shape[1]/(1+args.negative_K))\n",
    "for epoch in range(args.epochs):\n",
    "    tic_epoch = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    X, Y, Z= model()\n",
    "    predict_label=F.cosine_similarity(X[com_user_tensor_train[0]], X[com_user_tensor_train[1]])\n",
    "    #cos_loss = F.cosine_embedding_loss(com_user_tensor_train[0], com_user_tensor_train[1],)\n",
    "    #predict_label = F.sigmoid(predict_label)\n",
    "    #contrastive_loss = clloss(X_1, Y_1)\n",
    "    if(epoch%10==0):\n",
    "        print('EPOCH:',epoch,'####################################################')\n",
    "        print('predict_label',predict_label)\n",
    "        print('train_label',train_label)\n",
    "        #auc=sklearn.metrics.roc_auc_score(pl, tl)\n",
    "        #print(\"train auc\", auc)\n",
    "    cross_loss = F.binary_cross_entropy_with_logits(predict_label, train_label)\n",
    "    ssl1_loss = node_ssl_loss(args, X, Y)\n",
    "    ssl2_loss = com_ssl_loss(args, X, Z)\n",
    "    loss= cross_loss*args.lam_1 + ssl1_loss*args.lam_2+ssl2_loss*args.lam_3\n",
    "    print(\"Epoch:\",epoch,\"ssl2_loss\",ssl2_loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch>0:\n",
    "        if epoch%2==0:\n",
    "            auc, testl =test(X.cpu(), test_label, com_user_tensor_test)\n",
    "            print(\"Epoch:\",epoch, \"AUC:\",auc)\n",
    "            if auc>best_test_auc:\n",
    "                best_test_auc=auc\n",
    "                print(\"best_test_auc:\",best_test_auc)\n",
    "            print(\"best_test_auc:\",best_test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5743035c-b015-4e93-b0eb-3b092d2e4acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cd2447-accf-4a15-b165-1c128f389bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf144749-2cdd-434d-8452-d3a5344fb60c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c24c9f-0508-4acf-a1f9-a317b8522497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c66111-a336-450e-adbb-b502052d6696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b6280a-efa6-4aaf-9e34-c6325f5c65ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ecd692-68d2-4650-9c92-ce809458978b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f66241d-30aa-4939-9059-3cfb60205516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "toc-autonumbering": false,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
